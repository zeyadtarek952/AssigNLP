{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ae0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8820da89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv.csv',encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b64978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Empty Columns\n",
    "def  remove_empty_columns(df, threshold=0.9):\n",
    "    column_mask = df.isnull().mean(axis=0) < threshold\n",
    "    return df.loc[:, column_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4337d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = remove_empty_columns(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8171950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text):\n",
    "    return re.sub(r'http\\S+', ' ',text)\n",
    "df['CleanEmailText']=df['v2'].apply(clean_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4342009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df['CleanEmailText'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f03db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d037290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lowercase(text):\n",
    "    return(text).lower()\n",
    "\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(clean_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0663c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_alphanumeric(text):\n",
    "    return re.sub('[^a-zA-Z]',' ',text)\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(clean_non_alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba2c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_whitespces(text):\n",
    "    \n",
    "    return re.sub(\"\\s+\",' ',text)\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(rem_whitespces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad132eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19def83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5616f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0a6471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zeyad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0979982",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "def clean_stopwords(token):\n",
    "    return [item for item in token if item not in stop_words]\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ff58859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zeyad\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "def clean_lemmatization(token):\n",
    "    return[lemma.lemmatize(word=w,pos='v') for w in token]\n",
    "\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(clean_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca164ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanEmailText=[]\n",
    "def clean_lenght(token):\n",
    "    filtered_tokens=[]\n",
    "    for i in token:\n",
    "        if len(i)>=2:\n",
    "            filtered_tokens.append(i)\n",
    "    return filtered_tokens\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(clean_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f8c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [go, jurong, point, crazy, available, bugis, g...\n",
       "1                               [ok, lar, joke, wif, oni]\n",
       "2       [free, entry, wkly, comp, win, fa, cup, final,...\n",
       "3                    [dun, say, early, hor, already, say]\n",
       "4             [nah, think, go, usf, live, around, though]\n",
       "                              ...                        \n",
       "5567    [nd, time, try, contact, pound, prize, claim, ...\n",
       "5568                            [go, esplanade, fr, home]\n",
       "5569                            [pity, mood, suggestions]\n",
       "5570    [guy, bitch, act, like, interest, buy, somethi...\n",
       "5571                                   [rofl, true, name]\n",
       "Name: CleanEmailText, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CleanEmailText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f2e697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(listReview):\n",
    "    return '  '.join(listReview)\n",
    "\n",
    "df['CleanEmailText']=df['CleanEmailText'].apply(convert_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772156a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'freemsg  hey  darling  week  word  back  like  fun  still  tb  ok  xxx  std  chgs  send  rcv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CleanEmailText'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28a0b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['v1']\n",
    "X = df['CleanEmailText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af637a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd4518a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aah', 'aaooooright', ..., 'zouk', 'zs', 'zyada'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "775298fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9814593301435407\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy :\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "220a9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.32467583 0.         0.32467583 0.\n",
      "  0.32467583 0.         0.32467583 0.32467583 0.         0.\n",
      "  0.32467583 0.32467583 0.         0.         0.         0.\n",
      "  0.5119563  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.4472136\n",
      "  0.         0.         0.4472136  0.         0.4472136  0.\n",
      "  0.         0.4472136  0.         0.        ]\n",
      " [0.         0.39264414 0.         0.39264414 0.         0.39264414\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.6191303  0.         0.         0.39264414]\n",
      " [0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4472136  0.\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.         0.         0.4472136  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "  \"The quick brown fox jumps over the lazy dog.\",\n",
    "  \"A stitch in time saves nine.\",\n",
    "  \"The early bird catches the worm.\",\n",
    "  \"Actions speak louder than words.\",\n",
    "]\n",
    "\n",
    "# Create a TF-IDF vectorizer instance\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents\n",
    "tfidf.fit(docs)\n",
    "\n",
    "# Transform the documents into a TF-IDF matrix\n",
    "tfidf_matrix = tfidf.transform(docs)\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10718534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    actions      bird     brown   catches       dog     early       fox  \\\n",
      "0  0.000000  0.000000  0.324676  0.000000  0.324676  0.000000  0.324676   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.392644  0.000000  0.392644  0.000000  0.392644  0.000000   \n",
      "3  0.447214  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "         in     jumps      lazy  ...      over     quick     saves     speak  \\\n",
      "0  0.000000  0.324676  0.324676  ...  0.324676  0.324676  0.000000  0.000000   \n",
      "1  0.447214  0.000000  0.000000  ...  0.000000  0.000000  0.447214  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.447214   \n",
      "\n",
      "     stitch      than       the      time     words      worm  \n",
      "0  0.000000  0.000000  0.511956  0.000000  0.000000  0.000000  \n",
      "1  0.447214  0.000000  0.000000  0.447214  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.619130  0.000000  0.000000  0.392644  \n",
      "3  0.000000  0.447214  0.000000  0.000000  0.447214  0.000000  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "Index(['actions', 'bird', 'brown', 'catches', 'dog', 'early', 'fox', 'in',\n",
      "       'jumps', 'lazy', 'louder', 'nine', 'over', 'quick', 'saves', 'speak',\n",
      "       'stitch', 'than', 'the', 'time', 'words', 'worm'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert the matrix to a pandas dataframe\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Display the dataframe\n",
    "print(df_tfidf)\n",
    "print(df_tfidf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc039482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
